{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e3ca343",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mikol\\venvs\\Masters_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math, torch, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "260845ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_NAME = \"gpt2\"          #gpt2-medium, llama-2-7b-hf, etc.\n",
    "DS_NAME = (\"wikitext\", \"wikitext-103-v1\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aceeb7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n",
      "Moving model to device:  cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HookedTransformer(\n",
       "  (embed): Embed()\n",
       "  (hook_embed): HookPoint()\n",
       "  (pos_embed): PosEmbed()\n",
       "  (hook_pos_embed): HookPoint()\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x TransformerBlock(\n",
       "      (ln1): LayerNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (ln2): LayerNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (attn): Attention(\n",
       "        (hook_k): HookPoint()\n",
       "        (hook_q): HookPoint()\n",
       "        (hook_v): HookPoint()\n",
       "        (hook_z): HookPoint()\n",
       "        (hook_attn_scores): HookPoint()\n",
       "        (hook_pattern): HookPoint()\n",
       "        (hook_result): HookPoint()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (hook_pre): HookPoint()\n",
       "        (hook_post): HookPoint()\n",
       "      )\n",
       "      (hook_attn_in): HookPoint()\n",
       "      (hook_q_input): HookPoint()\n",
       "      (hook_k_input): HookPoint()\n",
       "      (hook_v_input): HookPoint()\n",
       "      (hook_mlp_in): HookPoint()\n",
       "      (hook_attn_out): HookPoint()\n",
       "      (hook_mlp_out): HookPoint()\n",
       "      (hook_resid_pre): HookPoint()\n",
       "      (hook_resid_mid): HookPoint()\n",
       "      (hook_resid_post): HookPoint()\n",
       "    )\n",
       "  )\n",
       "  (ln_final): LayerNormPre(\n",
       "    (hook_scale): HookPoint()\n",
       "    (hook_normalized): HookPoint()\n",
       "  )\n",
       "  (unembed): Unembed()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model.to(DEVICE).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9da6f9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:  \n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(*DS_NAME, split=\"test\")\n",
    "sample_text = dataset[\"text\"][0]            # first line\n",
    "prompt = sample_text \n",
    "print(\"Prompt:\", prompt[:120], \"...\" if len(prompt) > 120 else \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "503684e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, math\n",
    "\n",
    "def get_qkv_sa(model, prompt: str, device: str = \"cpu\"):\n",
    "    \"\"\"\n",
    "    Returns\n",
    "        traces: {layer: {\"Q\",\"K\",\"V\",\"S\",\"A\"}}\n",
    "        token_ids: list[int]\n",
    "    \"\"\"\n",
    "    # tokenise (no <BOS> so positions line up with prompt chars)\n",
    "    toks = model.to_tokens(prompt, prepend_bos=False)        # (1, L)\n",
    "    if toks.numel() == 0:\n",
    "        raise ValueError(\"Prompt produced zero tokens – supply some text.\")\n",
    "\n",
    "    # trim to context window and move to device as int64\n",
    "    toks = toks[:, -model.cfg.n_ctx :].to(device, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _, cache = model.run_with_cache(toks)\n",
    "\n",
    "    d_head = model.cfg.d_head\n",
    "    out = {}\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        q = cache[f\"blocks.{layer}.attn.hook_q\"][0]          # (H, L, d_h)\n",
    "        k = cache[f\"blocks.{layer}.attn.hook_k\"][0]\n",
    "        v = cache[f\"blocks.{layer}.attn.hook_v\"][0]\n",
    "        s = torch.einsum(\"hqd,hkd->hqk\", q, k) / math.sqrt(d_head)\n",
    "        a = s.softmax(-1)\n",
    "        out[layer] = {\"Q\": q, \"K\": k, \"V\": v, \"S\": s, \"A\": a}\n",
    "\n",
    "    return out, toks[0].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffc98f37",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Prompt produced zero tokens – supply some text.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m traces, token_ids = \u001b[43mget_qkv_sa\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m token_strs = tokenizer.convert_ids_to_tokens(token_ids)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mget_qkv_sa\u001b[39m\u001b[34m(model, prompt, device)\u001b[39m\n\u001b[32m     10\u001b[39m toks = model.to_tokens(prompt, prepend_bos=\u001b[38;5;28;01mFalse\u001b[39;00m)        \u001b[38;5;66;03m# (1, L)\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m toks.numel() == \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mPrompt produced zero tokens – supply some text.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# trim to context window and move to device as int64\u001b[39;00m\n\u001b[32m     15\u001b[39m toks = toks[:, -model.cfg.n_ctx :].to(device, dtype=torch.long)\n",
      "\u001b[31mValueError\u001b[39m: Prompt produced zero tokens – supply some text."
     ]
    }
   ],
   "source": [
    "traces, token_ids = get_qkv_sa(model, prompt, device=DEVICE)\n",
    "token_strs = tokenizer.convert_ids_to_tokens(token_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362f3370",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'traces' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m np.set_printoptions(precision=\u001b[32m4\u001b[39m, suppress=\u001b[38;5;28;01mTrue\u001b[39;00m, linewidth=\u001b[32m140\u001b[39m, threshold=\u001b[32m200\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer, layer_dict \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtraces\u001b[49m.items():\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m================  Layer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m  ================\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m     n_heads = layer_dict[\u001b[33m\"\u001b[39m\u001b[33mQ\u001b[39m\u001b[33m\"\u001b[39m].shape[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'traces' is not defined"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=4, suppress=True, linewidth=140, threshold=200)\n",
    "\n",
    "for layer, layer_dict in traces.items():\n",
    "    print(f\"\\n================  Layer {layer}  ================\\n\")\n",
    "    n_heads = layer_dict[\"Q\"].shape[0]\n",
    "\n",
    "    for head in range(n_heads):\n",
    "        print(f\"----------  Head {head}  ----------\")\n",
    "\n",
    "        for name in [\"Q\", \"K\", \"V\", \"S\", \"A\"]:\n",
    "            arr = layer_dict[name][head].cpu().numpy()\n",
    "            print(f\"{name}: shape {arr.shape}\")\n",
    "            print(arr, \"\\n\")           # comment out if too verbose\n",
    "\n",
    "        # labelled attention matrix\n",
    "        A = layer_dict[\"A\"][head].cpu().numpy()        # (seq, seq)\n",
    "        df_A = pd.DataFrame(A, index=token_strs, columns=token_strs)\n",
    "        print(\"Attention weights (rows = queries, cols = keys):\")\n",
    "        display(df_A.style.background_gradient(cmap=\"viridis\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a9bcf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Masters_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
