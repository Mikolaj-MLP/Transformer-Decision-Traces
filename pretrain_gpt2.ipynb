{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a475565",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "from src.data.load_csqa import load_csqa\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a8d35b",
   "metadata": {},
   "source": [
    "**Config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a7eb4ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 123\n",
    "MODEL_NAME = \"gpt2\"\n",
    "\n",
    "MAX_SEQ_LEN = 256      # prompt padding/truncation, could be seto to None\n",
    "TRAIN_LIMIT = None     # None = full train\n",
    "EVAL_LIMIT  = 256      # size of val set\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "GRAD_ACCUM = 1\n",
    "LR = 5e-5\n",
    "EPOCHS = 1.0\n",
    "WARMUP_RATIO = 0.03\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "USE_FP16 = True        # True for CUDA\n",
    "OUT_ROOT = Path(\"checkpoints\")\n",
    "\n",
    "set_seed(SEED)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9401088",
   "metadata": {},
   "source": [
    "**Load and examine Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e636e723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9741,\n",
       " 256,\n",
       " Index(['example_id', 'text', 'answerKey', 'correct_idx', 'csqa_choices'], dtype='object'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tr = load_csqa(split=\"train\", limit=TRAIN_LIMIT).reset_index(drop=True)\n",
    "df_va = load_csqa(split=\"validation\", limit=EVAL_LIMIT).reset_index(drop=True)\n",
    "\n",
    "len(df_tr), len(df_va), df_tr.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74382c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?\n",
      "Choices:\n",
      "A: ignore\n",
      "B: enforce\n",
      "C: authoritarian\n",
      "D: yell at\n",
      "E: avoid\n",
      "Answer:\n",
      "answerKey: A\n",
      "choices: [{'label': 'A', 'text': 'ignore'}, {'label': 'B', 'text': 'enforce'}, {'label': 'C', 'text': 'authoritarian'}, {'label': 'D', 'text': 'yell at'}, {'label': 'E', 'text': 'avoid'}]\n"
     ]
    }
   ],
   "source": [
    "print(df_tr.loc[0, \"text\"])\n",
    "print(\"answerKey:\", df_tr.loc[0, \"answerKey\"])\n",
    "print(\"choices:\", df_tr.loc[0, \"csqa_choices\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8918513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad_token_id: 50256 eos_token_id: 50256 padding_side: left\n"
     ]
    }
   ],
   "source": [
    "tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "# GPT-2 has no PAD token -> alias EOS as PAD\n",
    "if tok.pad_token_id is None and tok.eos_token_id is not None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "tok.padding_side = \"left\"\n",
    "\n",
    "print(\"pad_token_id:\", tok.pad_token_id, \"eos_token_id:\", tok.eos_token_id, \"padding_side:\", tok.padding_side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d251a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A [317] len= 1\n",
      "B [347] len= 1\n",
      "C [327] len= 1\n",
      "D [360] len= 1\n",
      "E [412] len= 1\n"
     ]
    }
   ],
   "source": [
    "answers = [\"A\",\"B\",\"C\",\"D\",\"E\"]\n",
    "for a in answers:\n",
    "    ids = tok(\" \" + a, add_special_tokens=False)[\"input_ids\"]\n",
    "    print(a, ids, \"len=\", len(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136ce523",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSQAGPT2Dataset(Dataset):\n",
    "    def __init__(self, texts, answer_keys, tok, max_len):\n",
    "        self.texts = texts\n",
    "        self.answer_keys = answer_keys\n",
    "        self.tok = tok\n",
    "        self.max_len = max_len\n",
    "\n",
    "        self.ans_token_id = {}\n",
    "        for a in [\"A\",\"B\",\"C\",\"D\",\"E\"]:\n",
    "            ids = tok(\" \" + a, add_special_tokens=False)[\"input_ids\"]\n",
    "            assert len(ids) == 1, f\"Answer {a} not 1 token: {ids}\"\n",
    "            self.ans_token_id[a] = ids[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        ak = self.answer_keys[idx]\n",
    "\n",
    "        enc = self.tok(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "\n",
    "        input_ids = enc[\"input_ids\"][0]\n",
    "        attention_mask = enc[\"attention_mask\"][0]\n",
    "\n",
    "        last_pos = int(attention_mask.sum().item()) - 1\n",
    "        if last_pos < 0:\n",
    "            last_pos = 0\n",
    "\n",
    "        labels = torch.full_like(input_ids, -100)\n",
    "        labels[last_pos] = self.ans_token_id[ak]\n",
    "\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "\n",
    "class SimpleCollator:\n",
    "    def __call__(self, features):\n",
    "        out = {}\n",
    "        for k in features[0].keys():\n",
    "            out[k] = torch.stack([f[k] for f in features], dim=0)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1337fed0",
   "metadata": {},
   "source": [
    "**Sanity check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d456c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids len: torch.Size([256]) mask sum: 54\n",
      "supervised positions: [53]\n",
      "label token id: 317\n",
      "label token:  A\n"
     ]
    }
   ],
   "source": [
    "ds = CSQAGPT2Dataset(df_tr[\"text\"].tolist(), df_tr[\"answerKey\"].tolist(), tok, MAX_SEQ_LEN)\n",
    "ex = ds[0]\n",
    "\n",
    "print(\"input_ids len:\", ex[\"input_ids\"].shape, \"mask sum:\", int(ex[\"attention_mask\"].sum()))\n",
    "pos = (ex[\"labels\"] != -100).nonzero().flatten().tolist()\n",
    "print(\"supervised positions:\", pos)\n",
    "\n",
    "p = pos[0]\n",
    "print(\"label token id:\", int(ex[\"labels\"][p]))\n",
    "print(\"label token:\", tok.decode([int(ex[\"labels\"][p])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f024c7",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a326fa",
   "metadata": {},
   "source": [
    "**Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bfad5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "model.resize_token_embeddings(len(tok))\n",
    "model.to(device)\n",
    "\n",
    "train_ds = CSQAGPT2Dataset(df_tr[\"text\"].tolist(), df_tr[\"answerKey\"].tolist(), tok, MAX_SEQ_LEN)\n",
    "eval_ds  = CSQAGPT2Dataset(df_va[\"text\"].tolist(), df_va[\"answerKey\"].tolist(), tok, MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82fc3f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    labels = labels.astype(np.int64)\n",
    "    B, T, V = logits.shape\n",
    "    acc = []\n",
    "    for i in range(B):\n",
    "        pos = np.where(labels[i] != -100)[0]\n",
    "        if len(pos) != 1:\n",
    "            continue\n",
    "        p = int(pos[0])\n",
    "        y = int(labels[i, p])\n",
    "        pred = int(np.argmax(logits[i, p]))\n",
    "        acc.append(1.0 if pred == y else 0.0)\n",
    "    return {\"acc\": float(np.mean(acc)) if acc else 0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f74bfaf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('checkpoints/20260113-142203_gpt2_csqa_ft')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_id = f\"{time.strftime('%Y%m%d-%H%M%S')}_{MODEL_NAME}_csqa_ft\"\n",
    "out_dir = OUT_ROOT / run_id\n",
    "out_dir                                  # Output folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0038b9d4",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1221454",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_fp16 = bool(USE_FP16 and torch.cuda.is_available())\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=str(out_dir),\n",
    "    overwrite_output_dir=True,\n",
    "\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "\n",
    "    learning_rate=LR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    "\n",
    "    fp16=use_fp16,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    data_collator=SimpleCollator(),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a299a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mikol\\venvs\\Masters_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='332' max='1218' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 332/1218 2:24:16 < 6:27:22, 0.04 it/s, Epoch 0.27/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m train_metrics = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.metrics\n\u001b[32m      2\u001b[39m eval_metrics = trainer.evaluate()\n\u001b[32m      4\u001b[39m trainer.save_model(\u001b[38;5;28mstr\u001b[39m(out_dir))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mikol\\venvs\\Masters_env\\Lib\\site-packages\\transformers\\trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mikol\\venvs\\Masters_env\\Lib\\site-packages\\transformers\\trainer.py:2560\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2553\u001b[39m context = (\n\u001b[32m   2554\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2555\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2556\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2557\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2558\u001b[39m )\n\u001b[32m   2559\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2560\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2562\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2563\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2564\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2565\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2566\u001b[39m ):\n\u001b[32m   2567\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2568\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mikol\\venvs\\Masters_env\\Lib\\site-packages\\transformers\\trainer.py:3782\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   3779\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   3780\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3782\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3784\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mikol\\venvs\\Masters_env\\Lib\\site-packages\\accelerate\\accelerator.py:2454\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2452\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2453\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2454\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mikol\\venvs\\Masters_env\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mikol\\venvs\\Masters_env\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mikol\\venvs\\Masters_env\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train_metrics = trainer.train().metrics\n",
    "eval_metrics = trainer.evaluate()\n",
    "\n",
    "trainer.save_model(str(out_dir))\n",
    "tok.save_pretrained(str(out_dir))\n",
    "\n",
    "meta = {\n",
    "    \"run_id\": run_id,\n",
    "    \"base_model\": MODEL_NAME,\n",
    "    \"dataset\": \"csqa\",\n",
    "    \"train_n\": len(df_tr),\n",
    "    \"eval_n\": len(df_va),\n",
    "    \"max_seq_len\": MAX_SEQ_LEN,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"grad_accum\": GRAD_ACCUM,\n",
    "    \"lr\": LR,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"fp16\": use_fp16,\n",
    "    \"train_metrics\": {k: float(v) for k, v in train_metrics.items() if isinstance(v, (int,float))},\n",
    "    \"eval_metrics\": {k: float(v) for k, v in eval_metrics.items() if isinstance(v, (int,float))},\n",
    "    \"time\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "}\n",
    "with open(out_dir / \"finetune_meta.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "print(\"[done] saved:\", out_dir)\n",
    "print(\"eval:\", eval_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf86a8cf",
   "metadata": {},
   "source": [
    "**Post training checkup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f1d4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "sample = df_va.loc[0, \"text\"]\n",
    "enc = tok(sample, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=MAX_SEQ_LEN, add_special_tokens=False)\n",
    "enc = {k: v.to(device) for k,v in enc.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model(**enc)\n",
    "logits = out.logits[0]  # (T,V)\n",
    "\n",
    "last_pos = int(enc[\"attention_mask\"][0].sum().item()) - 1\n",
    "top = torch.topk(logits[last_pos], k=10)\n",
    "\n",
    "print(\"Prompt:\", sample)\n",
    "print(\"True:\", df_va.loc[0, \"answerKey\"])\n",
    "print(\"Top preds:\")\n",
    "for score, tid in zip(top.values, top.indices):\n",
    "    print(float(score), repr(tok.decode([int(tid)])))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Masters_env (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
