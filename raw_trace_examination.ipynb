{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd83bc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.load_csqa import load_csqa\n",
    "from src.traces_utils.store import TraceStore\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b047ccb",
   "metadata": {},
   "source": [
    "**Data creation using extract_trace_csqa_gpt.py**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde7fb12",
   "metadata": {},
   "source": [
    "python -m src.cli.extract_trace_csqa_gpt --split validation --limit 8 --batch_size 2 --max_seq_len auto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0729d5",
   "metadata": {},
   "source": [
    "- here 8 rows for test purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a9fde8",
   "metadata": {},
   "source": [
    "**Dane**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d26ad8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['example_id', 'text', 'answerKey', 'correct_idx', 'csqa_choices'], dtype='object')\n",
      "Q: A revolving door is convenient for two direction travel, but it also serves as a security measure at a what?\n",
      "Choices:\n",
      "A: bank\n",
      "B: library\n",
      "C: department store\n",
      "D: mall\n",
      "E: new york\n",
      "Answer:\n",
      "A\n",
      "[{'label': 'A', 'text': 'bank'}, {'label': 'B', 'text': 'library'}, {'label': 'C', 'text': 'department store'}, {'label': 'D', 'text': 'mall'}, {'label': 'E', 'text': 'new york'}]\n"
     ]
    }
   ],
   "source": [
    "df = load_csqa(\"validation\", limit=3)\n",
    "print(df.columns)\n",
    "print(df.loc[0, \"text\"])\n",
    "print(df.loc[0, \"answerKey\"])\n",
    "print(df.loc[0, \"csqa_choices\"])\n",
    "assert isinstance(df.loc[0,\"csqa_choices\"], list) and len(df.loc[0,\"csqa_choices\"]) == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a81bbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_dir = Path(r\"traces\\20260112-175054_gpt2_csqa_validation_n8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5057be0d",
   "metadata": {},
   "source": [
    "**parquet file check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ce8d7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['example_id', 'text', 'input_ids', 'attention_mask', 'offset_mapping',\n",
      "       'tokens', 'answerKey', 'csqa_choices'],\n",
      "      dtype='object')\n",
      "rows: 8 T: 54\n",
      "input_ids lens min/max: 54 54\n",
      "mask sums: count     8.000000\n",
      "mean     47.625000\n",
      "std       3.852179\n",
      "min      43.000000\n",
      "25%      44.750000\n",
      "50%      47.000000\n",
      "75%      49.750000\n",
      "max      54.000000\n",
      "Name: attention_mask, dtype: float64\n",
      "Q: A revolving door is convenient for two direction travel, but it also serves as a security measure at a what?\n",
      "Choices:\n",
      "A: bank\n",
      "B: library\n",
      "C: department store\n",
      "D: mall\n",
      "E: new york\n",
      "Answer:\n",
      "[{'label': 'A', 'text': 'bank'} {'label': 'B', 'text': 'library'}\n",
      " {'label': 'C', 'text': 'department store'} {'label': 'D', 'text': 'mall'}\n",
      " {'label': 'E', 'text': 'new york'}]\n"
     ]
    }
   ],
   "source": [
    "meta = json.load(open(run_dir/\"meta.json\"))\n",
    "T = meta[\"max_seq_len\"]\n",
    "\n",
    "df_tok = pd.read_parquet(run_dir/\"tokens.parquet\")\n",
    "print(df_tok.columns)\n",
    "print(\"rows:\", len(df_tok), \"T:\", T)\n",
    "\n",
    "# kluczowe sanity:\n",
    "lens = df_tok[\"input_ids\"].apply(len)\n",
    "print(\"input_ids lens min/max:\", lens.min(), lens.max())\n",
    "print(\"mask sums:\", df_tok[\"attention_mask\"].apply(sum).describe())\n",
    "\n",
    "print(df_tok.loc[0, \"text\"])\n",
    "print(df_tok.loc[0, \"csqa_choices\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0093de8",
   "metadata": {},
   "source": [
    "**Zarr check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab86a1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'run_id': '20260112-175054_gpt2_csqa_validation_n8', 'model': 'gpt2', 'arch': 'dec', 'dataset': 'csqa', 'split': 'validation', 'n_examples': 8, 'max_seq_len': 54, 'num_layers': 12, 'num_heads': 12, 'head_dim': 64, 'dtype': 'float16', 'capture': ['attn', 'qkv', 'hidden', 'resid'], 'has_targets': None, 'time': '2026-01-12 17:51:03'}\n",
      "{'dec_self_attn': (8, 12, 12, 54, 54), 'dec_self_q': (8, 12, 12, 54, 64), 'dec_self_k': (8, 12, 12, 54, 64), 'dec_self_v': (8, 12, 12, 54, 64), 'dec_hidden': (8, 13, 54, 768), 'dec_res_embed': (8, 54, 768), 'dec_res_pre_attn': (8, 12, 54, 768), 'dec_res_post_attn': (8, 12, 54, 768), 'dec_res_post_mlp': (8, 12, 54, 768)}\n",
      "attn shape: (54, 54) nan: 0\n"
     ]
    }
   ],
   "source": [
    "store = TraceStore(run_dir)\n",
    "print(store.meta)\n",
    "print(store.arrays())\n",
    "\n",
    "eid = store.tokens.loc[0, \"example_id\"]\n",
    "A = store.attn(eid, side=\"dec\", kind=\"self\", layer=0, head=0)  # (T,T)\n",
    "print(\"attn shape:\", A.shape, \"nan:\", np.isnan(A).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35cd373",
   "metadata": {},
   "source": [
    "**Sanity-check: recompute attention from stored Q,K and compare to stored attn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832a272e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ex_id: 701fac8b8c04ab56c4394b2e7b2aa8df\n",
      "T: 54 mask.sum: 54\n",
      "arrays: {'dec_self_attn': (8, 12, 12, 54, 54), 'dec_self_q': (8, 12, 12, 54, 64), 'dec_self_k': (8, 12, 12, 54, 64), 'dec_self_v': (8, 12, 12, 54, 64), 'dec_hidden': (8, 13, 54, 768), 'dec_res_embed': (8, 54, 768), 'dec_res_pre_attn': (8, 12, 54, 768), 'dec_res_post_attn': (8, 12, 54, 768), 'dec_res_post_mlp': (8, 12, 54, 768)}\n",
      "[L0 H0] max_abs=0.00012804 mean_abs=4.81548e-06 rmse=1.18393e-05 | row_sum(calc) min/mean/max=1/1/1 | row_sum(stored) min/mean/max=0.999891/1/1.00015\n",
      "[L0 H5] max_abs=0.000257051 mean_abs=3.26902e-06 rmse=1.93978e-05 | row_sum(calc) min/mean/max=1/1/1 | row_sum(stored) min/mean/max=0.999753/1.00001/1.0003\n",
      "[L5 H0] max_abs=0.000787193 mean_abs=6.44366e-06 rmse=3.91147e-05 | row_sum(calc) min/mean/max=1/1/1 | row_sum(stored) min/mean/max=0.999755/0.999991/1.00024\n",
      "[L11 H11] max_abs=0.000344378 mean_abs=5.38771e-06 rmse=2.37184e-05 | row_sum(calc) min/mean/max=1/1/1 | row_sum(stored) min/mean/max=0.999687/0.999982/1.00025\n",
      "\n",
      "Summary:\n",
      "max_abs_all: 0.0007871930038847452\n",
      "mean_abs_all: 4.978968531475942e-06\n",
      "All good, stored attention matches attention recomputed from stored Q,K (within float16 tolerance).\n"
     ]
    }
   ],
   "source": [
    "# example index of a transformer decision trace\n",
    "ex_id = store.tokens.loc[0, \"example_id\"]\n",
    "enc = store.encodings(ex_id)\n",
    "mask = np.array(enc[\"attention_mask\"], dtype=np.int64)  \n",
    "T = mask.shape[0]\n",
    "\n",
    "print(\"ex_id:\", ex_id)\n",
    "print(\"T:\", T, \"mask.sum:\", int(mask.sum()))\n",
    "print(\"arrays:\", store.arrays())\n",
    "\n",
    "def causal_mask(T: int) -> np.ndarray:\n",
    "    # allow attend to self and past: j <= i\n",
    "    return np.tril(np.ones((T, T), dtype=bool))\n",
    "\n",
    "def pad_key_mask(mask_1d: np.ndarray) -> np.ndarray:\n",
    "    # keys with mask=0 are blocked for all Qs\n",
    "    return (mask_1d.astype(bool)[None, :])  # shape (1, T) broadcast over Qs\n",
    "\n",
    "def softmax_np(x: np.ndarray, axis=-1) -> np.ndarray:  # stable softmax\n",
    "    x = x - np.max(x, axis=axis, keepdims=True)\n",
    "    ex = np.exp(x)\n",
    "    return ex / np.sum(ex, axis=axis, keepdims=True)\n",
    "\n",
    "def attn_from_qk(q: np.ndarray, k: np.ndarray, mask_1d: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    q,k: (T,d) numpy\n",
    "    returns: (T,T) attention probs (float64) matching GPT-2 masking:\n",
    "      - causal mask \n",
    "      - padding keys blocked (mask=0)\n",
    "      - only compares active query rows.\n",
    "    \"\"\"\n",
    "    d = q.shape[-1]\n",
    "    scores = (q @ k.T) / np.sqrt(d)  # (T,T)\n",
    "\n",
    "    # build combined mask: allowed positions True\n",
    "    allow = causal_mask(T) & pad_key_mask(mask_1d)  # (T,T) via broadcast\n",
    "    # set disallowed to very negative\n",
    "    scores = np.where(allow, scores, -1e9)\n",
    "\n",
    "    probs = softmax_np(scores, axis=-1)  # (T,T)\n",
    "    return probs\n",
    "\n",
    "def compare_one(layer: int, head: int, verbose=True):\n",
    "    # stored arrays\n",
    "    A_stored = store.attn(ex_id, layer=layer, head=head, side=\"dec\", kind=\"self\")  # (T,T)\n",
    "    q = store.qkv(ex_id, which=\"q\", layer=layer, head=head, side=\"dec\", kind=\"self\")  # (T,d)\n",
    "    k = store.qkv(ex_id, which=\"k\", layer=layer, head=head, side=\"dec\", kind=\"self\")  # (T,d)\n",
    "\n",
    "    # recompute\n",
    "    A_calc = attn_from_qk(q.astype(np.float64), k.astype(np.float64), mask)\n",
    "\n",
    "    # compare only on active query rows so ones that are real tokens\n",
    "    q_active = mask.astype(bool)\n",
    "    diff = A_calc[q_active] - A_stored[q_active].astype(np.float64)\n",
    "\n",
    "    max_abs = float(np.max(np.abs(diff)))\n",
    "    mean_abs = float(np.mean(np.abs(diff)))\n",
    "    rmse = float(np.sqrt(np.mean(diff**2)))\n",
    "\n",
    "    # row-sum sanity\n",
    "    row_sum_calc = A_calc[q_active].sum(axis=-1)\n",
    "    row_sum_stored = A_stored[q_active].astype(np.float64).sum(axis=-1)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[L{layer} H{head}] max_abs={max_abs:.6g} mean_abs={mean_abs:.6g} rmse={rmse:.6g} \"\n",
    "              f\"| row_sum(calc) min/mean/max={row_sum_calc.min():.6g}/{row_sum_calc.mean():.6g}/{row_sum_calc.max():.6g} \"\n",
    "              f\"| row_sum(stored) min/mean/max={row_sum_stored.min():.6g}/{row_sum_stored.mean():.6g}/{row_sum_stored.max():.6g}\")\n",
    "\n",
    "    return {\"layer\": layer, \"head\": head, \"max_abs\": max_abs, \"mean_abs\": mean_abs, \"rmse\": rmse}\n",
    "\n",
    "# test a few ]\n",
    "tests = [(0,0), (0,5), (5,0), (11,11)]\n",
    "results = [compare_one(L,H) for (L,H) in tests]\n",
    "\n",
    "# stricter pass/fail heuristic for float16 traces:\n",
    "# expected small differences due to float16 number format + potential attention scaling/masking conventions\n",
    "# acceptable if : max_abs <= 1e-2, mean_abs <= 1e-4..1e-3\n",
    "max_abs_all = max(r[\"max_abs\"] for r in results)\n",
    "mean_abs_all = sum(r[\"mean_abs\"] for r in results)/len(results)\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "print(\"max_abs_all:\", max_abs_all)\n",
    "print(\"mean_abs_all:\", mean_abs_all)\n",
    "\n",
    "if max_abs_all < 5e-2 and mean_abs_all < 5e-3:\n",
    "    print(\"All good, stored attention matches attention recomputed from stored Q,K (within tolerance).\")\n",
    "else:\n",
    "    print(\"Potential mismatch, differences are larger than expected.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Masters_env (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
